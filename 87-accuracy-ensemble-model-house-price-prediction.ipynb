{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing necessary libraries","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:23:16.640146Z","iopub.execute_input":"2024-06-25T21:23:16.641377Z","iopub.status.idle":"2024-06-25T21:23:16.676889Z","shell.execute_reply.started":"2024-06-25T21:23:16.641330Z","shell.execute_reply":"2024-06-25T21:23:16.675759Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:22:42.694110Z","iopub.execute_input":"2024-07-06T17:22:42.694480Z","iopub.status.idle":"2024-07-06T17:22:42.700778Z","shell.execute_reply.started":"2024-07-06T17:22:42.694450Z","shell.execute_reply":"2024-07-06T17:22:42.699189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the dataset...","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:22:42.703131Z","iopub.execute_input":"2024-07-06T17:22:42.703633Z","iopub.status.idle":"2024-07-06T17:22:42.741035Z","shell.execute_reply.started":"2024-07-06T17:22:42.703591Z","shell.execute_reply":"2024-07-06T17:22:42.739624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)","metadata":{}},{"cell_type":"code","source":"from IPython.display import display, HTML\n\nprint(\"Train data shape:\", train_data.shape)\n\n# Display the first few rows of train_data in a table\ntrain_table_html = train_data.head().to_html()\n\n# Display the table\ndisplay(HTML(\"<h3>Train Data</h3>\"))\ndisplay(HTML(train_table_html))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:22:42.743464Z","iopub.execute_input":"2024-07-06T17:22:42.744066Z","iopub.status.idle":"2024-07-06T17:22:42.781824Z","shell.execute_reply.started":"2024-07-06T17:22:42.744009Z","shell.execute_reply":"2024-07-06T17:22:42.780184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:22:42.783556Z","iopub.execute_input":"2024-07-06T17:22:42.784139Z","iopub.status.idle":"2024-07-06T17:22:42.795638Z","shell.execute_reply.started":"2024-07-06T17:22:42.784096Z","shell.execute_reply":"2024-07-06T17:22:42.794109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\nThis refrence helped me a lot https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python.","metadata":{}},{"cell_type":"code","source":"#descriptive statistics summary\ntrain_data['SalePrice'].describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:22:42.797113Z","iopub.execute_input":"2024-07-06T17:22:42.797477Z","iopub.status.idle":"2024-07-06T17:22:42.812179Z","shell.execute_reply.started":"2024-07-06T17:22:42.797448Z","shell.execute_reply":"2024-07-06T17:22:42.810873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#histogram\nsns.distplot(train_data['SalePrice']);","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:22:42.813722Z","iopub.execute_input":"2024-07-06T17:22:42.814421Z","iopub.status.idle":"2024-07-06T17:22:43.321395Z","shell.execute_reply.started":"2024-07-06T17:22:42.814216Z","shell.execute_reply":"2024-07-06T17:22:43.319711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Deviate from the normal distribution.\n* Have appreciable positive skewness.\n* Show peakedness.","metadata":{}},{"cell_type":"code","source":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train_data['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_data['SalePrice'].kurt())","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:22:43.322937Z","iopub.execute_input":"2024-07-06T17:22:43.323355Z","iopub.status.idle":"2024-07-06T17:22:43.331785Z","shell.execute_reply.started":"2024-07-06T17:22:43.323325Z","shell.execute_reply":"2024-07-06T17:22:43.330296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Relationship with numerical variables","metadata":{}},{"cell_type":"code","source":"#scatter plot grlivarea/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:22:43.333513Z","iopub.execute_input":"2024-07-06T17:22:43.333881Z","iopub.status.idle":"2024-07-06T17:22:43.736915Z","shell.execute_reply.started":"2024-07-06T17:22:43.333846Z","shell.execute_reply":"2024-07-06T17:22:43.735357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that `SalePrice` and `GrLivArea` have a linear relationship","metadata":{}},{"cell_type":"code","source":"#scatter plot totalbsmtsf/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:22:43.738691Z","iopub.execute_input":"2024-07-06T17:22:43.739105Z","iopub.status.idle":"2024-07-06T17:22:44.150467Z","shell.execute_reply.started":"2024-07-06T17:22:43.739071Z","shell.execute_reply":"2024-07-06T17:22:44.149178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Relationship with categorical features","metadata":{}},{"cell_type":"code","source":"#box plot overallqual/saleprice\nvar = 'OverallQual'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:22:44.151916Z","iopub.execute_input":"2024-07-06T17:22:44.152490Z","iopub.status.idle":"2024-07-06T17:22:44.703779Z","shell.execute_reply.started":"2024-07-06T17:22:44.152454Z","shell.execute_reply":"2024-07-06T17:22:44.701761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var = 'YearBuilt'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:22:44.705568Z","iopub.execute_input":"2024-07-06T17:22:44.706141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: we don't know if `SalePrice` is in constant prices. Constant prices try to remove the effect of inflation. If `SalePrice` is not in constant prices, it should be, so than prices are comparable over the years","metadata":{}},{"cell_type":"markdown","source":"In summary\n, we can conclude that:\n\n`GrLivArea` and `TotalBsmtSF` seem to be linearly related with `SalePrice`. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of `TotalBsmtSF`, we can see that the slope of the linear relationship is particularly high.\n`OverallQual` and 'YearBuilt' also seem to be related with `SalePrice`. The relationship seems to be stronger in the case of `OverallQual`, where the box plot shows how sales prices increase with the overall quality.\nWe just analysed four variables, but there are many other that we should analyse. The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).","metadata":{}},{"cell_type":"markdown","source":"* `SalePrice` correlation matrix (zoomed heatmap style).\n \n* Scatter plots between the most correlated variables.","metadata":{}},{"cell_type":"markdown","source":"#### 'SalePrice' correlation matrix (zoomed heatmap style)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.graph_objects as go\n\n# Check if 'SalePrice' column exists\nif 'SalePrice' not in train_data.columns:\n    raise ValueError(\"'SalePrice' column is missing in the dataset\")\n\n# Select numerical and categorical columns\nnumerical_cols = train_data.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols = train_data.select_dtypes(include=['object']).columns\n\n# Encode categorical variables (one-hot encoding)\nencoded_categorical_cols = pd.get_dummies(train_data[categorical_cols])\n\n# Concatenate numerical and encoded categorical columns\nencoded_data = pd.concat([train_data[numerical_cols], encoded_categorical_cols], axis=1)\n\n# Calculate correlation matrix\ncorr_matrix = encoded_data.corr().abs()\n\n# Select top 10 most correlated features\ntarget = 'SalePrice'\ntop_corr_features = corr_matrix[target].sort_values(ascending=False).head(21).index.tolist()\n\n# Filter correlation matrix to include only top features\nfiltered_corr_matrix = corr_matrix.loc[top_corr_features, top_corr_features]\n\n# Create annotated heatmap using Plotly's Heatmap trace\nfig = go.Figure(data=go.Heatmap(\n    z=filtered_corr_matrix.values,\n    x=top_corr_features,\n    y=top_corr_features,\n    colorscale='RdYlGn',\n    colorbar=dict(title='Correlation'),\n    hoverongaps=False\n))\n\nfig.update_layout(\n    title='Top 20 Most Correlated Features with SalePrice Heatmap',\n    title_x=0.5,\n    xaxis=dict(tickangle=45, automargin=True),\n    yaxis=dict(tickangle=0, automargin=True),\n    width=800,\n    height=800,\n    font=dict(size=12)\n)\n\n# Add hover information (correlation values)\nhover_text = []\nfor i in range(len(top_corr_features)):\n    row = []\n    for j in range(len(top_corr_features)):\n        row.append(f'Correlation: {filtered_corr_matrix.values[i][j]:.2f}<br>{top_corr_features[i]} vs {top_corr_features[j]}')\n    hover_text.append(row)\n\nfig.update_traces(hoverinfo='text', text=hover_text, hovertemplate='%{text}')\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Scatter plots between 'SalePrice' and correlated variables","metadata":{}},{"cell_type":"code","source":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_data[cols], size = 2.5)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing data","metadata":{}},{"cell_type":"code","source":"#missing data\ntotal = train_data.isnull().sum().sort_values(ascending=False)\npercent = (train_data.isnull().sum()/train_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's analyse this to understand how to handle the missing data.\n\nWe'll consider that when more than 15% of the data is missing, we should delete the corresponding variable and pretend it never existed. This means that we will not try any trick to fill the missing data in these cases. According to this, there is a set of variables (e.g. 'PoolQC', 'MiscFeature', 'Alley', etc.) that we should delete. The point is: will we miss this data? I don't think so. None of these variables seem to be very important, since most of them are not aspects in which we think about when buying a house (maybe that's the reason why data is missing?). Moreover, looking closer at the variables, we could say that variables like 'PoolQC', 'MiscFeature' and 'FireplaceQu' are strong candidates for outliers, so we'll be happy to delete them.\n\n\nRegarding 'MasVnrArea' and 'MasVnrType', we can consider that these variables are not essential. Furthermore, they have a strong correlation with 'YearBuilt' and 'OverallQual' which are already considered. Thus, we will not lose information if we delete 'MasVnrArea' and 'MasVnrType'.\n\nFinally, we have one missing observation in 'Electrical'. Since it is just one observation, we'll delete this observation and keep the variable.\n\nIn summary, to handle missing data, we'll delete all the variables with missing data, except the variable 'Electrical'. In 'Electrical' we'll just delete the observation with missing data.","metadata":{}},{"cell_type":"code","source":"# Identify columns with any missing data\nmissing_data = train_data.isnull().sum()\n\n# Drop all columns with missing data except 'Electrical'\ncols_to_drop = missing_data[missing_data > 0].index.difference(['Electrical'])\ntrain_data = train_data.drop(columns=cols_to_drop)\n\n# Drop rows where 'Electrical' has missing data\ntrain_data = train_data.dropna(subset=['Electrical'])\n\ntrain_data.isnull().sum().max() #just checking that there's no missing data missing...","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outliers\n\nOutliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.\n\nOutliers is a complex subject and it deserves more attention. Here, we'll just do a quick analysis through the standard deviation of `SalePrice` and a set of scatter plots.","metadata":{}},{"cell_type":"markdown","source":"### Univariate analysis\n\nThe primary concern here is to establish a threshold that defines an observation as an outlier. To do so, we'll standardize the data. In this context, data standardization means converting data values to have mean of 0 and a standard deviation of 1.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(np.array(train_data['SalePrice']).reshape(-1, 1));\nlow_range = saleprice_scaled[saleprice_scaled[:, 0].argsort()][:10]\nhigh_range = saleprice_scaled[saleprice_scaled[:, 0].argsort()][-10:]\n\nprint('Outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nOuter range (high) of the distribution:')\nprint(high_range)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Low range values are similar and not too far from 0.\n* High range values are far from 0 and the 7.something values are really out of range.\n\nFor now, we'll not consider any of these values as an outlier but we should be careful with those values.","metadata":{}},{"cell_type":"markdown","source":"### Bivariate analysis","metadata":{}},{"cell_type":"code","source":"#bivariate analysis saleprice/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What has been revealed:\n\n* The two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.\n\n* The two observations in the top of the plot are those 7.something observations that we said we should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them.","metadata":{}},{"cell_type":"code","source":"#deleting points\ntrain_data.sort_values(by = 'GrLivArea', ascending = False)[:2]\ntrain_data = train_data.drop(train_data[train_data['Id'] == 1299].index)\ntrain_data = train_data.drop(train_data[train_data['Id'] == 524].index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bivariate analysis saleprice/GrLivArea\nvar = 'TotalBsmtSF'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#deleting points\nvar = 'GrLivArea'\nfiltered_data = train_data[train_data[var] <= 3000]\ndata = pd.concat([filtered_data['SalePrice'], filtered_data[var]], axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normality\n> When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this.\n> we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.","metadata":{"execution":{"iopub.status.busy":"2024-06-28T13:56:20.148223Z","iopub.execute_input":"2024-06-28T13:56:20.149032Z","iopub.status.idle":"2024-06-28T13:56:20.153267Z","shell.execute_reply.started":"2024-06-28T13:56:20.148996Z","shell.execute_reply":"2024-06-28T13:56:20.152362Z"}}},{"cell_type":"markdown","source":"The point here is to test `SalePrice` in a very lean way. We'll do this paying attention to:\n\n* Histogram - Kurtosis and skewness.\n\n* Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, probplot\n\n# Histogram and normal probability plot\nsns.histplot(train_data['SalePrice'], kde=True, stat=\"density\", linewidth=0, element='step', bins=30, color='blue')\nsns.lineplot(x=sorted(train_data['SalePrice']), y=norm.pdf(sorted(train_data['SalePrice']), loc=train_data['SalePrice'].mean(), scale=train_data['SalePrice'].std()), color='red')\n\nfig = plt.figure()\nres = probplot(train_data['SalePrice'], plot=plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* `SalePrice` is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.\n*  A simple data transformation can solve the problem. in case of positive skewness, log transformations usually works well.","metadata":{}},{"cell_type":"code","source":"#applying log transformation\ntrain_data['SalePrice'] = np.log(train_data['SalePrice'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformed histogram and normal probability plot\nsns.histplot(train_data['SalePrice'], kde=True, stat=\"density\", bins=30, color='blue')\nmean, std = norm.fit(train_data['SalePrice'])\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean, std)\nplt.plot(x, p, 'k', linewidth=2, color='red')\n\nfig = plt.figure()\nres = probplot(train_data['SalePrice'], plot=plt)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Done! Let's check what's going on with `GrLivArea`.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, probplot\nimport numpy as np\n\n# Histogram and normal probability plot\nsns.histplot(train_data['GrLivArea'], kde=True, stat=\"density\", bins=30, color='blue')\nmean, std = norm.fit(train_data['GrLivArea'])\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean, std)\nplt.plot(x, p, 'k', linewidth=2, color='red')\n\nfig = plt.figure()\nres = probplot(train_data['GrLivArea'], plot=plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* skewness deteted ! ","metadata":{}},{"cell_type":"code","source":"#data transformation\ntrain_data['GrLivArea'] = np.log(train_data['GrLivArea'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, probplot\nimport numpy as np\n\n# Histogram and normal probability plot\nsns.histplot(train_data['GrLivArea'], kde=True, stat=\"density\", bins=30, color='blue')\nmean, std = norm.fit(train_data['GrLivArea'])\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean, std)\nplt.plot(x, p, 'k', linewidth=2, color='red')\n\nfig = plt.figure()\nres = probplot(train_data['GrLivArea'], plot=plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to check `TotalBsmtSF`","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, probplot\nimport numpy as np\n\n# Histogram and normal probability plot\nsns.histplot(train_data['TotalBsmtSF'], kde=True, stat=\"density\", bins=30, color='blue')\nmean, std = norm.fit(train_data['TotalBsmtSF'])\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean, std)\nplt.plot(x, p, 'k', linewidth=2, color='red')\n\nfig = plt.figure()\nres = probplot(train_data['TotalBsmtSF'], plot=plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Skewness is present\n* A significant number of observations with value zero (houses without basement).\n* A big problem because the value zero doesn't allow us to do log transformations.","metadata":{}},{"cell_type":"markdown","source":"To apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.","metadata":{}},{"cell_type":"code","source":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ntrain_data['HasBsmt'] = pd.Series(len(train_data['TotalBsmtSF']), index=train_data.index)\ntrain_data['HasBsmt'] = 0 \ntrain_data.loc[train_data['TotalBsmtSF']>0,'HasBsmt'] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transform data\ntrain_data.loc[train_data['HasBsmt']==1,'TotalBsmtSF'] = np.log(train_data['TotalBsmtSF'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, probplot\nimport numpy as np\n\n# Filtering out the zero values in 'TotalBsmtSF'\nfiltered_data = train_data[train_data['TotalBsmtSF'] > 0]['TotalBsmtSF']\n\n# Histogram and normal probability plot\nsns.histplot(filtered_data, kde=True, stat=\"density\", bins=30, color='blue')\nmean, std = norm.fit(filtered_data)\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean, std)\nplt.plot(x, p, 'k', linewidth=2, color='red')\nplt.title('Histogram with Normal Fit')\n\nfig = plt.figure()\nres = probplot(filtered_data, plot=plt)\nplt.title('Normal Probability Plot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Homoscedasticity\n\n> Homoscedasticity refers to the assumption that \"dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)\" (Hair et al., 2013). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.","metadata":{}},{"cell_type":"markdown","source":"The best approach to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).\n\nStarting by 'SalePrice' and 'GrLivArea'...","metadata":{}},{"cell_type":"code","source":"#scatter plot\nplt.scatter(train_data['GrLivArea'], train_data['SalePrice']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's check `SalePrice` with `TotalBsmtSF`.","metadata":{}},{"cell_type":"code","source":"#scatter plot\nplt.scatter(train_data[train_data['TotalBsmtSF']>0]['TotalBsmtSF'], train_data[train_data['TotalBsmtSF']>0]['SalePrice']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can say that, in general, `SalePrice` exhibit equal levels of variance across the range of `TotalBsmtSF`. Cool!","metadata":{}},{"cell_type":"markdown","source":"## Dummy Variables\n\n[https://medium.com/analytics-vidhya/tutorial-exploratory-data-analysis-eda-with-categorical-variables-6a569a3aea55](http://)","metadata":{}},{"cell_type":"code","source":"#convert categorical variable into dummy\ntrain_data = pd.get_dummies(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model\n[https://www.kaggle.com/code/pavansanagapati/ensemble-learning-techniques-tutorial/notebook#Please-do-leave-your-comments-/suggestions-and-if-you-like-this-kernel-greatly-appreciate-to-UPVOTE-.](http://)","metadata":{}},{"cell_type":"markdown","source":"### Model Selection and Hyperparameter Tuning :\n\n* **Choosing Models** : Various regression algorithms are selected based on their suitability for the task, including Lasso, ElasticNet, KernelRidge, GradientBoostingRegressor, XGBRegressor, LGBMRegressor, and MLPRegressor.\n\n\n* **Hyperparameter Optimization** : Each model undergoes hyperparameter tuning using RandomizedSearchCV. \n\n This involves :\n   \n   * Defining a hyperparameter grid (param_grids) specific to each model.\n    \n   *   Conducting randomized searches over the parameter grid to find the best combination that maximizes model performance, measured by cross-validated metrics.\n\n\n### Ensemble Learning - Stacking :\n\n* **Constructing StackingRegressor** : The best-tuned base models are combined using a StackingRegressor.\n\n     This meta-estimator :\n      \n    * Aggregates predictions from multiple base models.\n    \n    * Uses a final estimator (e.g., Ridge, SVR, XGBRegressor, LGBMRegressor) to blend the predictions of base models into a single output.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\nX_train_cv, X_val_cv, y_train_cv, y_val_cv = train_test_split(train.drop('SalePrice', axis=1), train['SalePrice'], test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Based on the distribution of data let us remove some of the outliers","metadata":{}},{"cell_type":"code","source":"train.drop(train[(train['GrLivArea'] >4000) & (train['SalePrice']<300000)].index,inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full = pd.concat([train, X_val_cv], ignore_index=True)\nfull.drop('Id',axis = 1,inplace = True)\nfull.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = full.isnull().sum()\nmissing_values[missing_values>0].sort_values(ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let us impute the missing values of LotFrontage based on the median of LotArea and Neighborhood. To achieve this let us first group Neighborhood and LotFrontage with respect to median,mean and count.","metadata":{}},{"cell_type":"code","source":"full.groupby(['Neighborhood'])[['LotFrontage']].agg(['mean','median','count'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LotArea is a continuous feature so it is best to use panda's qcut method to divide it into 10 parts.","metadata":{}},{"cell_type":"code","source":"full['LotAreaCut'] = pd.qcut(full.LotArea,10)\n\nfull.groupby([full['LotAreaCut']])[['LotFrontage']].agg(['mean','median','count'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So let us impute the missing values of LotFrontage as stated above with the median of LotArea and Neighborhood.","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:33:49.134033Z","iopub.execute_input":"2024-07-04T20:33:49.134848Z","iopub.status.idle":"2024-07-04T20:33:49.142805Z","shell.execute_reply.started":"2024-07-04T20:33:49.134811Z","shell.execute_reply":"2024-07-04T20:33:49.141218Z"}}},{"cell_type":"code","source":"full['LotFrontage']= full.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\nfull['LotFrontage']= full.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us recheck the missing values to see our LotFrontage missing values are imputed successfully.","metadata":{}},{"cell_type":"code","source":"missing_values = full.isnull().sum()\n\nmissing_values[missing_values>0].sort_values(ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"focus on numerical features with one missing value and replace them with 0","metadata":{}},{"cell_type":"code","source":"columns = [\"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"GarageCars\", \"BsmtFinSF2\", \"BsmtFinSF1\", \"GarageArea\"]\nfor col in columns:full[col].fillna(0,inplace= True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"focus on some of the categorical features with major count of missing values and replace them with 'None'","metadata":{}},{"cell_type":"code","source":"columns1 = [\"PoolQC\" , \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"GarageFinish\",\n\"GarageYrBlt\", \"GarageType\", \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\", \"MasVnrType\"]\nfor col1 in columns1:full[col1].fillna('None',inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"focus on some of the categorical features with fewer missing values and replace them with the most frequently occured value which is the mode of that feature.","metadata":{}},{"cell_type":"code","source":"columns2 = [\"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\", \"Functional\",\n            \"Electrical\", \"KitchenQual\", \"SaleType\",\"Exterior1st\", \"Exterior2nd\"]\n\nfor col2 in columns2:\n    full[col2].fillna(full[col2].mode()[0],inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"recheck if we have any other missing values that needs to be imputed except the SalePrice for the test dataset which is the target variable to be determined.","metadata":{}},{"cell_type":"code","source":"full.isnull().sum()[full.isnull().sum()>0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_features = full.select_dtypes(include=[np.number])\nnumeric_features.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Numstr = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"MoSold\",\n          \"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"LowQualFinSF\",\"GarageYrBlt\"]\n\nfor i in Numstr:\n    full[i]=full[i].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full.groupby(['MSSubClass'])[['SalePrice']].agg(['mean','median','count'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_values():\n    full[\"oMSSubClass\"] = full.MSSubClass.map({'180':1, \n                                        '30':2, '45':2, \n                                        '190':3, '50':3, '90':3, \n                                        '85':4, '40':4, '160':4, \n                                        '70':5, '20':5, '75':5, '80':5, '150':5,\n                                        '120': 6, '60':6})\n    \n    full[\"oMSZoning\"] = full.MSZoning.map({'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4})\n    full[\"oNeighborhood\"] = full.Neighborhood.map({'MeadowV':1,\n                                               'IDOTRR':2, 'BrDale':2,\n                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,\n                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n                                               'NPkVill':5, 'Mitchel':5,\n                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,\n                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n                                               'Veenker':8, 'Somerst':8, 'Timber':8,\n                                               'StoneBr':9,\n                                               'NoRidge':10, 'NridgHt':10})\n    \n    full[\"oCondition1\"] = full.Condition1.map({'Artery':1,\n                                           'Feedr':2, 'RRAe':2,\n                                           'Norm':3, 'RRAn':3,\n                                           'PosN':4, 'RRNe':4,\n                                           'PosA':5 ,'RRNn':5})\n    \n    full[\"oBldgType\"] = full.BldgType.map({'2fmCon':1, 'Duplex':1, 'Twnhs':1, '1Fam':2, 'TwnhsE':2})\n    \n    full[\"oHouseStyle\"] = full.HouseStyle.map({'1.5Unf':1, \n                                           '1.5Fin':2, '2.5Unf':2, 'SFoyer':2, \n                                           '1Story':3, 'SLvl':3,\n                                           '2Story':4, '2.5Fin':4})\n    \n    full[\"oExterior1st\"] = full.Exterior1st.map({'BrkComm':1,\n                                             'AsphShn':2, 'CBlock':2, 'AsbShng':2,\n                                             'WdShing':3, 'Wd Sdng':3, 'MetalSd':3, 'Stucco':3, 'HdBoard':3,\n                                             'BrkFace':4, 'Plywood':4,\n                                             'VinylSd':5,\n                                             'CemntBd':6,\n                                             'Stone':7, 'ImStucc':7})\n    \n    full[\"oMasVnrType\"] = full.MasVnrType.map({'BrkCmn':1, 'None':1, 'BrkFace':2, 'Stone':3})\n    \n    full[\"oExterQual\"] = full.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n    \n    full[\"oFoundation\"] = full.Foundation.map({'Slab':1, \n                                           'BrkTil':2, 'CBlock':2, 'Stone':2,\n                                           'Wood':3, 'PConc':4})\n    \n    full[\"oBsmtQual\"] = full.BsmtQual.map({'Fa':2, 'None':1, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    full[\"oBsmtExposure\"] = full.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n    \n    full[\"oHeating\"] = full.Heating.map({'Floor':1, 'Grav':1, 'Wall':2, 'OthW':3, 'GasW':4, 'GasA':5})\n    \n    full[\"oHeatingQC\"] = full.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    full[\"oKitchenQual\"] = full.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n    \n    full[\"oFunctional\"] = full.Functional.map({'Maj2':1, 'Maj1':2, 'Min1':2, 'Min2':2, 'Mod':2, 'Sev':2, 'Typ':3})\n    \n    full[\"oFireplaceQu\"] = full.FireplaceQu.map({'None':1, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    full[\"oGarageType\"] = full.GarageType.map({'CarPort':1, 'None':1,\n                                           'Detchd':2,\n                                           '2Types':3, 'Basment':3,\n                                           'Attchd':4, 'BuiltIn':5})\n    \n    full[\"oGarageFinish\"] = full.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n    \n    full[\"oPavedDrive\"] = full.PavedDrive.map({'N':1, 'P':2, 'Y':3})\n    \n    full[\"oSaleType\"] = full.SaleType.map({'COD':1, 'ConLD':1, 'ConLI':1, 'ConLw':1, 'Oth':1, 'WD':1,\n                                       'CWD':2, 'Con':3, 'New':3})\n    \n    full[\"oSaleCondition\"] = full.SaleCondition.map({'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4})            \n                \n                        \n                        \n    \n    return \"Done!\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop two unwanted columns\nfull.drop(\"LotAreaCut\",axis=1,inplace=True)\n\nfull.drop(['SalePrice'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create a class for the LabelEncoder to fit and transform some of the identified features","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator,TransformerMixin\n\nclass labenc(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        label = LabelEncoder()\n        X['YearBuilt']=label.fit_transform(X['YearBuilt'])\n        X['YearRemodAdd']=label.fit_transform(X['YearRemodAdd'])\n        X['GarageYrBlt']=label.fit_transform(X['GarageYrBlt'])\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class skewness(BaseEstimator,TransformerMixin):\n    def __init__(self,skew=0.5):\n        self.skew = skew\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        X_numeric=X.select_dtypes(exclude=[\"object\"])\n        skewness = X_numeric.apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= self.skew].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class dummies(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        X = pd.get_dummies(X)\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will use pipeline to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves two purposes here:\n\nConvenience: You only have to call fit and predict once on your data to fit a whole sequence of estimators. Joint parameter selection: You can grid search over parameters of all estimators in the pipeline at once. All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([('labenc',labenc()),('skewness',skewness(skew =1)),('dummies',dummies())])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import skew\n\nfull_copy = full.copy()\ndata_pipeline = pipeline.fit_transform(full_copy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nrobust_scaler = RobustScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_train = train.shape[0]\nn_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X= data_pipeline[:n_train]\ny = train.SalePrice\ntest_X = data_pipeline[n_train:]\nX.shape,y.shape,test_X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scaled = robust_scaler.fit(X).transform(X)\ny_log = np.log(train.SalePrice)\ntest_X_scaled = robust_scaler.transform(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scaled.shape,y_log.shape,test_X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will perform some feature selection like Lasso","metadata":{}},{"cell_type":"code","source":"class add_feature(BaseEstimator, TransformerMixin):\n    def __init__(self,additional=1):\n        self.additional = additional\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.additional==1:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            \n        else:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            \n            X[\"+_TotalHouse_OverallQual\"] = X[\"TotalHouse\"] * X[\"OverallQual\"]\n            X[\"+_GrLivArea_OverallQual\"] = X[\"GrLivArea\"] * X[\"OverallQual\"]\n            X[\"+_oMSZoning_TotalHouse\"] = X[\"oMSZoning\"] * X[\"TotalHouse\"]\n            X[\"+_oMSZoning_OverallQual\"] = X[\"oMSZoning\"] + X[\"OverallQual\"]\n            X[\"+_oMSZoning_YearBuilt\"] = X[\"oMSZoning\"] + X[\"YearBuilt\"]\n            X[\"+_oNeighborhood_TotalHouse\"] = X[\"oNeighborhood\"] * X[\"TotalHouse\"]\n            X[\"+_oNeighborhood_OverallQual\"] = X[\"oNeighborhood\"] + X[\"OverallQual\"]\n            X[\"+_oNeighborhood_YearBuilt\"] = X[\"oNeighborhood\"] + X[\"YearBuilt\"]\n            X[\"+_BsmtFinSF1_OverallQual\"] = X[\"BsmtFinSF1\"] * X[\"OverallQual\"]\n            \n            X[\"-_oFunctional_TotalHouse\"] = X[\"oFunctional\"] * X[\"TotalHouse\"]\n            X[\"-_oFunctional_OverallQual\"] = X[\"oFunctional\"] + X[\"OverallQual\"]\n            X[\"-_LotArea_OverallQual\"] = X[\"LotArea\"] * X[\"OverallQual\"]\n            X[\"-_TotalHouse_LotArea\"] = X[\"TotalHouse\"] + X[\"LotArea\"]\n            X[\"-_oCondition1_TotalHouse\"] = X[\"oCondition1\"] * X[\"TotalHouse\"]\n            X[\"-_oCondition1_OverallQual\"] = X[\"oCondition1\"] + X[\"OverallQual\"]\n            \n           \n            X[\"Bsmt\"] = X[\"BsmtFinSF1\"] + X[\"BsmtFinSF2\"] + X[\"BsmtUnfSF\"]\n            X[\"Rooms\"] = X[\"FullBath\"]+X[\"TotRmsAbvGrd\"]\n            X[\"PorchArea\"] = X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n            X[\"TotalPlace\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"] + X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n\n    \n            return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline = Pipeline([('labenc',labenc()),('add_feature', add_feature(additional=2)),\n                     ('skewness',skewness(skew =1)),('dummies',dummies())])\n\nfull_pipe = pipeline.fit_transform(full)\nfull_pipe.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_train=train.shape[0]\nX = full_pipe[:n_train]\ntest_X = full_pipe[n_train:]\ny= train.SalePrice\n\nX_scaled = robust_scaler.fit(X).transform(X)\ny_log = np.log(train.SalePrice)\ntest_X_scaled = robust_scaler.transform(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_scaled.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let us define Root Mean Square Error \ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model,X,y,scoring=\"neg_mean_squared_error\",cv=5))\n    return rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We choose 4 models and use 5-folds cross-calidation to evaluate these models.\n\n**Models include** : \n* LinearRegression\n* Ridge\n* Lasso\n* Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor\n\nmodels = [LinearRegression(),\n             Ridge(),\n             Lasso(alpha=0.01,max_iter=10000),\n             RandomForestRegressor(),\n             GradientBoostingRegressor(),\n             SVR(),\n             LinearSVR(),\n             ElasticNet(alpha = 0.001,max_iter=10000),\n             SGDRegressor(max_iter=1000, tol = 1e-3),\n             BayesianRidge(),\n             KernelRidge(alpha=0.6,kernel='polynomial',degree = 2,coef0=2.5),\n             ExtraTreesRegressor(),\n             XGBRegressor()\n             ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = ['LR','Ridge','Lasso','RF','GBR','SVR','LSVR','ENet','SGDR','BayRidge','Kernel','XTreeR','XGBR']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nfor model,name in zip(models,names):\n    score = rmse_cv(model,X_scaled,y_log)\n    print(\"{}: {:.6f}, {:4f}\".format(name,score.mean(),score.std()))","metadata":{"execution":{"iopub.status.idle":"2024-07-06T17:24:29.738673Z","shell.execute_reply.started":"2024-07-06T17:23:18.813386Z","shell.execute_reply":"2024-07-06T17:24:29.737654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import RegressorMixin\n\n# To define the average weight \nclass AverageWeight(BaseEstimator, RegressorMixin):\n    def __init__(self,model,weight):\n        self.model = model\n        self.weight = weight\n        \n    def fit(self,X,y):\n        self.models_ = [clone(x) for x in self.model]\n        for model in self.models_:\n            model.fit(X,y)\n        return self\n    \n    def predict(self,X):\n        w = list()\n        pred = np.array([model.predict(X) for model in self.models_])\n        # for every data point, single model prediction times weight, then add them together\n        for data in range(pred.shape[1]):\n            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n            w.append(np.sum(single))\n        return w","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:24:29.741209Z","iopub.execute_input":"2024-07-06T17:24:29.741657Z","iopub.status.idle":"2024-07-06T17:24:29.753674Z","shell.execute_reply.started":"2024-07-06T17:24:29.741625Z","shell.execute_reply":"2024-07-06T17:24:29.752326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso = Lasso(alpha= 0.0005, max_iter= 10000)\nridge = Ridge(alpha=45, max_iter= 10000)\nsvr = SVR(C = 0.2, epsilon= 0.025, gamma = 0.0004, kernel = 'rbf')\nker = KernelRidge(alpha=0.15 ,kernel='polynomial',degree=3 , coef0=0.9)\nela = ElasticNet(alpha=0.0065,l1_ratio=0.075,max_iter=10000)\nbay = BayesianRidge()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:24:29.754888Z","iopub.execute_input":"2024-07-06T17:24:29.755289Z","iopub.status.idle":"2024-07-06T17:24:29.772376Z","shell.execute_reply.started":"2024-07-06T17:24:29.755256Z","shell.execute_reply":"2024-07-06T17:24:29.771065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally to calculate the average weights let us look at the following code","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:40:24.452737Z","iopub.execute_input":"2024-07-04T20:40:24.453190Z","iopub.status.idle":"2024-07-04T20:40:24.460423Z","shell.execute_reply.started":"2024-07-04T20:40:24.453155Z","shell.execute_reply":"2024-07-04T20:40:24.459045Z"}}},{"cell_type":"code","source":"from sklearn.base import clone\n\n# Assign weights to all the above 6 models\nw1 = 0.047\nw2 = 0.2\nw3 = 0.25\nw4 = 0.3\nw5 = 0.003\nw6 = 0.2\n\nweight_avg = AverageWeight(model = [lasso,ridge,svr,ker,ela,bay],weight=[w1,w2,w3,w4,w5,w6])\nscore = rmse_cv(weight_avg,X_scaled,y_log)\nprint(score.mean())","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:24:29.774035Z","iopub.execute_input":"2024-07-06T17:24:29.774443Z","iopub.status.idle":"2024-07-06T17:24:37.678414Z","shell.execute_reply.started":"2024-07-06T17:24:29.774412Z","shell.execute_reply":"2024-07-06T17:24:37.677146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we consider only two models then the score will vary","metadata":{}},{"cell_type":"code","source":"weight_avg = AverageWeight(model = [svr,ker],weight=[0.50,0.50])\nscore = rmse_cv(weight_avg,X_scaled,y_log)\nprint(score.mean())","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:24:37.680091Z","iopub.execute_input":"2024-07-06T17:24:37.680998Z","iopub.status.idle":"2024-07-06T17:24:40.364593Z","shell.execute_reply.started":"2024-07-06T17:24:37.680930Z","shell.execute_reply":"2024-07-06T17:24:40.363058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking\nStacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. This model is used for making predictions on the test set.","metadata":{}},{"cell_type":"code","source":"# Define the stacking class\nclass stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, mod, meta_model):\n        self.mod = mod\n        self.meta_model = meta_model\n        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)\n        \n    def fit(self, X, y):\n        self.saved_model = [[] for _ in self.mod]\n        oof_train = np.zeros((X.shape[0], len(self.mod)))\n        \n        for i, model in enumerate(self.mod):\n            for train_index, val_index in self.kf.split(X, y):\n                renew_model = clone(model)\n                renew_model.fit(X[train_index], y[train_index])\n                self.saved_model[i].append(renew_model)\n                oof_train[val_index, i] = renew_model.predict(X[val_index])\n        \n        self.meta_model.fit(oof_train, y)\n        return self\n    \n    def predict(self, X):\n        whole_test = np.column_stack([np.column_stack([model.predict(X) for model in single_model]).mean(axis=1) \n                                      for single_model in self.saved_model]) \n        return self.meta_model.predict(whole_test)\n    \n    def get_oof(self, X, y, test_X):\n        oof = np.zeros((X.shape[0], len(self.mod)))\n        test_single = np.zeros((test_X.shape[0], 5))\n        test_mean = np.zeros((test_X.shape[0], len(self.mod)))\n        for i, model in enumerate(self.mod):\n            for j, (train_index, val_index) in enumerate(self.kf.split(X, y)):\n                clone_model = clone(model)\n                clone_model.fit(X[train_index], y[train_index])\n                oof[val_index, i] = clone_model.predict(X[val_index])\n                test_single[:, j] = clone_model.predict(test_X)\n            test_mean[:, i] = test_single.mean(axis=1)\n        return oof, test_mean","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:24:40.366051Z","iopub.execute_input":"2024-07-06T17:24:40.367300Z","iopub.status.idle":"2024-07-06T17:24:40.390403Z","shell.execute_reply.started":"2024-07-06T17:24:40.367251Z","shell.execute_reply":"2024-07-06T17:24:40.388890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom sklearn.base import clone\nfrom sklearn.model_selection import KFold\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\n\nX_scaled_imputed = SimpleImputer().fit_transform(X_scaled)\ny_log_imputed = SimpleImputer().fit_transform(y_log.values.reshape(-1,1)).ravel()\n\n# Initialize stacking model\nstack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)\nscore = rmse_cv(stack_model,X_scaled_imputed,y_log_imputed)\nprint(score.mean())","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:24:40.391979Z","iopub.execute_input":"2024-07-06T17:24:40.392463Z","iopub.status.idle":"2024-07-06T17:25:13.763304Z","shell.execute_reply.started":"2024-07-06T17:24:40.392421Z","shell.execute_reply":"2024-07-06T17:25:13.762139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Blending\nBlending follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. In other words, unlike stacking, the predictions are made on the holdout set only. The holdout set and the predictions are used to build a model which is run on the test set. ","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_wine\n# define dataset\nX,y = load_wine().data,load_wine().target","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:13.765278Z","iopub.execute_input":"2024-07-06T17:25:13.766155Z","iopub.status.idle":"2024-07-06T17:25:13.777651Z","shell.execute_reply.started":"2024-07-06T17:25:13.766115Z","shell.execute_reply":"2024-07-06T17:25:13.776372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=0.25, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:13.779584Z","iopub.execute_input":"2024-07-06T17:25:13.780555Z","iopub.status.idle":"2024-07-06T17:25:13.789156Z","shell.execute_reply.started":"2024-07-06T17:25:13.780516Z","shell.execute_reply":"2024-07-06T17:25:13.787872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_val=pd.DataFrame(X_val)\nx_test=pd.DataFrame(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:13.791245Z","iopub.execute_input":"2024-07-06T17:25:13.792143Z","iopub.status.idle":"2024-07-06T17:25:13.799145Z","shell.execute_reply.started":"2024-07-06T17:25:13.792092Z","shell.execute_reply":"2024-07-06T17:25:13.797872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel1 = DecisionTreeClassifier()\nmodel1.fit(X_train, y_train)\nval_pred1=model1.predict(X_val)\ntest_pred1=model1.predict(X_test)\nval_pred1=pd.DataFrame(val_pred1)\ntest_pred1=pd.DataFrame(test_pred1)\n\nmodel2 = KNeighborsClassifier()\nmodel2.fit(X_train,y_train)\nval_pred2=model2.predict(X_val)\ntest_pred2=model2.predict(X_test)\nval_pred2=pd.DataFrame(val_pred2)\ntest_pred2=pd.DataFrame(test_pred2)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:13.803106Z","iopub.execute_input":"2024-07-06T17:25:13.805687Z","iopub.status.idle":"2024-07-06T17:25:13.836271Z","shell.execute_reply.started":"2024-07-06T17:25:13.805630Z","shell.execute_reply":"2024-07-06T17:25:13.835015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\ndf_val=pd.concat([x_val, val_pred1,val_pred2],axis=1)\ndf_test=pd.concat([x_test, test_pred1,test_pred2],axis=1)\n\nmodel = LogisticRegression()\nmodel.fit(df_val,y_val)\nmodel.score(df_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:13.840280Z","iopub.execute_input":"2024-07-06T17:25:13.841270Z","iopub.status.idle":"2024-07-06T17:25:13.887743Z","shell.execute_reply.started":"2024-07-06T17:25:13.841228Z","shell.execute_reply":"2024-07-06T17:25:13.886809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bagging\nBagging, is shorthand for the combination of bootstrapping and aggregating. Bootstrapping is a method to help decrease the variance of the classifier and reduce overfitting, by resampling data from the training set with the same cardinality as the original set. The model created should be less overfitted than a single individual model.","metadata":{}},{"cell_type":"markdown","source":"There are three main terms describing the ensemble (combination) of various models into one more effective model:\n\n* **Bagging** to decrease the model’s variance;\n* **Boosting** to decreasing the model’s bias, and;\n* **Stacking** to increasing the predictive force of the classifier.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_wine\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor, VotingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# define dataset\nX,y = load_wine().data,load_wine().target","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:13.889244Z","iopub.execute_input":"2024-07-06T17:25:13.889870Z","iopub.status.idle":"2024-07-06T17:25:13.898367Z","shell.execute_reply.started":"2024-07-06T17:25:13.889833Z","shell.execute_reply":"2024-07-06T17:25:13.897371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Boosting\nThe main idea of boosting is to add additional models to the overall ensemble model sequentially.\n\nPreviously with bagging, we averaged each individual model created. This time with each iteration of boosting, a new model is created and the new base-learner model is trained (updated) from the errors of the previous learners.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_wine\n# define dataset\nX,y = load_wine().data,load_wine().target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:13.899777Z","iopub.execute_input":"2024-07-06T17:25:13.900174Z","iopub.status.idle":"2024-07-06T17:25:13.914466Z","shell.execute_reply.started":"2024-07-06T17:25:13.900138Z","shell.execute_reply":"2024-07-06T17:25:13.913320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Adaptive boosting** or **AdaBoost** is one of the simplest boosting algorithms.\nUsually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model.\n\nAdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nada_boost = AdaBoostRegressor(random_state=1)\nada_boost.fit(X_train, y_train)\nada_boost.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:13.915929Z","iopub.execute_input":"2024-07-06T17:25:13.916343Z","iopub.status.idle":"2024-07-06T17:25:14.010884Z","shell.execute_reply.started":"2024-07-06T17:25:13.916302Z","shell.execute_reply":"2024-07-06T17:25:14.009920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Gradient Boosting or GBM**\n\nIt is another ensemble machine learning algorithm that works for both regression and classification problems.\n\nGBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngrad_boost = GradientBoostingRegressor(learning_rate=0.01, random_state=1)\ngrad_boost.fit(X_train, y_train)\ngrad_boost.score(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:14.012056Z","iopub.execute_input":"2024-07-06T17:25:14.012372Z","iopub.status.idle":"2024-07-06T17:25:14.115810Z","shell.execute_reply.started":"2024-07-06T17:25:14.012346Z","shell.execute_reply":"2024-07-06T17:25:14.113890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBRegressor\n\n\nxgb_boost = XGBRegressor(random_state=1, learning_rate=0.01)\nxgb_boost.fit(X_train, y_train)\nxgb_boost.score(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:14.117249Z","iopub.execute_input":"2024-07-06T17:25:14.117591Z","iopub.status.idle":"2024-07-06T17:25:14.199566Z","shell.execute_reply.started":"2024-07-06T17:25:14.117563Z","shell.execute_reply":"2024-07-06T17:25:14.198655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n# Define RMSE scorer (root mean squared error)\nscorer = make_scorer(mean_squared_error, squared=False)\n\neclf = VotingRegressor(estimators=[\n    ('Ada Boost', ada_boost),\n    ('Grad Boost', grad_boost),\n    ('XG Boost', xgb_boost)], \n    weights=[1,1,1])\n\n# List of regressors\nregressors = [ada_boost, grad_boost, xgb_boost, eclf]\n\n# Loop through regressors and evaluate using RMSE\nfor reg, label in zip(regressors, ['Ada Boost', 'Grad Boost', 'XG Boost', 'Ensemble']):\n    scores = cross_val_score(reg, X, y, cv=10, scoring=scorer)\n    print(\"RMSE: %0.2f (+/- %0.2f) [%s]\" % (-scores.mean(), scores.std(), label))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:14.202092Z","iopub.execute_input":"2024-07-06T17:25:14.202833Z","iopub.status.idle":"2024-07-06T17:25:19.295114Z","shell.execute_reply.started":"2024-07-06T17:25:14.202798Z","shell.execute_reply":"2024-07-06T17:25:19.294137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eclf.fit(X_train, y_train)\n\n# Evaluate using cross-validation and RMSE\nscores = cross_val_score(eclf, X, y, cv=10, scoring=scorer)\nprint(\"RMSE: %0.2f (+/- %0.2f) [%s]\" % (-scores.mean(), scores.std(), 'Ensemble'))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:19.296351Z","iopub.execute_input":"2024-07-06T17:25:19.297471Z","iopub.status.idle":"2024-07-06T17:25:22.209320Z","shell.execute_reply.started":"2024-07-06T17:25:19.297437Z","shell.execute_reply":"2024-07-06T17:25:22.208044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:22.210693Z","iopub.execute_input":"2024-07-06T17:25:22.211487Z","iopub.status.idle":"2024-07-06T17:25:22.230432Z","shell.execute_reply.started":"2024-07-06T17:25:22.211431Z","shell.execute_reply":"2024-07-06T17:25:22.227901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nimport numpy as np\n\nclass CustomVotingRegressor(VotingRegressor):\n    def __init__(self, estimators, weights):\n        super().__init__(estimators, weights)\n    \n    def predict(self, X):\n        # Assuming self.saved_model and self.meta_model are defined appropriately\n        whole_test = np.column_stack([np.column_stack([model.predict(X) for model in single_model]).mean(axis=1) \n                                      for single_model in self.saved_model]) \n        return self.meta_model.predict(whole_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:22.232272Z","iopub.execute_input":"2024-07-06T17:25:22.232654Z","iopub.status.idle":"2024-07-06T17:25:22.241817Z","shell.execute_reply.started":"2024-07-06T17:25:22.232623Z","shell.execute_reply":"2024-07-06T17:25:22.240232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Given metrics\nstack_rmse = 0.1076\nscore_et = 0.92\nscore_ada = 0.95\n\n# Calculate inverse RMSE for stacking model\ninv_rmse_stack = 1 / stack_rmse\n\n# Normalize scores for Extra Trees and AdaBoost\nnormalized_et = score_et / (score_et + score_ada)\nnormalized_ada = score_ada / (score_et + score_ada)\n\n# Assign weights\nweight_stack = inv_rmse_stack / (inv_rmse_stack + normalized_et + normalized_ada)\nweight_et = normalized_et / (inv_rmse_stack + normalized_et + normalized_ada)\nweight_ada = normalized_ada / (inv_rmse_stack + normalized_et + normalized_ada)\n\nprint(f\"Weight for stacking model: {weight_stack:.3f}\")\nprint(f\"Weight for Extra Trees model: {weight_et:.3f}\")\nprint(f\"Weight for AdaBoost model: {weight_ada:.3f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:22.243319Z","iopub.execute_input":"2024-07-06T17:25:22.243726Z","iopub.status.idle":"2024-07-06T17:25:22.258507Z","shell.execute_reply.started":"2024-07-06T17:25:22.243692Z","shell.execute_reply":"2024-07-06T17:25:22.257202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import BaggingRegressor, ExtraTreesRegressor, VotingRegressor, AdaBoostRegressor\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.base import clone\n\n# Load the dataset\ntrain = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\n# Define the cross-validation set\nX_train_cv, X_val_cv, y_train_cv, y_val_cv = train_test_split(train.drop('SalePrice', axis=1), train['SalePrice'], test_size=0.2, random_state=42)\n\n# Combine the train and cross-validation sets\nfull = pd.concat([X_train_cv, X_val_cv], ignore_index=True)\nfull.drop('Id', axis=1, inplace=True)\n\ntest_ID = test['Id']\n\n# Drop 'Id' column from the combined dataset\nprint(\"Shape of the combined dataset:\", full.shape)\n\n# Identify numeric and categorical columns\nnumeric_features = full.select_dtypes(include=[np.number]).columns\ncategorical_features = full.select_dtypes(exclude=[np.number]).columns\n\n# Create preprocessing pipelines for numeric and categorical features\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Preprocess the training and test datasets\nX_train_df = pd.DataFrame(X_train_cv)\ny_log_series = np.log1p(y_train_cv)\n\n# Reset indices\nX_train_df.reset_index(drop=True, inplace=True)\ny_log_series.reset_index(drop=True, inplace=True)\nX_val_cv.reset_index(drop=True, inplace=True)\n\nX_train_preprocessed = preprocessor.fit_transform(X_train_df)\nX_test_preprocessed = preprocessor.transform(test.drop('Id', axis=1))\n\n# Define the models\nlasso = Lasso(alpha=0.0005, max_iter=10000)\nridge = Ridge(alpha=45, max_iter=10000)\nsvr = SVR(C=0.2, epsilon=0.025, gamma=0.0004, kernel='rbf')\nker = KernelRidge(alpha=0.15, kernel='polynomial', degree=3, coef0=0.9)\nela = ElasticNet(alpha=0.0065, l1_ratio=0.075, max_iter=10000)\nbay = BayesianRidge()\n\n# Average weight model\nclass AverageWeight:\n    def __init__(self, model, weight):\n        self.model = model\n        self.weight = weight\n\n    def fit(self, X, y):\n        for m in self.model:\n            m.fit(X, y)\n    \n    def predict(self, X):\n        weighted_sum = 0\n        for m, w in zip(self.model, self.weight):\n            weighted_sum += w * m.predict(X)\n        return weighted_sum\n\nweight_avg = AverageWeight(model=[lasso, ridge, svr, ker, ela, bay], weight=[0.047, 0.2, 0.25, 0.3, 0.003, 0.2])\n\n# Stacking model\nclass Stacking:\n    def __init__(self, mod, meta_model):\n        self.mod = mod\n        self.meta_model = meta_model\n        self.kf = KFold(n_splits=5, shuffle=True, random_state=42)\n        self.saved_model = [[] for _ in mod]\n    \n    def fit(self, X, y):\n        oof_train = np.zeros((X.shape[0], len(self.mod)))\n        for i, model in enumerate(self.mod):\n            for train_index, val_index in self.kf.split(X, y):\n                renew_model = clone(model)\n                renew_model.fit(X[train_index], y[train_index])\n                self.saved_model[i].append(renew_model)\n                oof_train[val_index, i] = renew_model.predict(X[val_index])\n        self.meta_model.fit(oof_train, y)\n    \n    def predict(self, X):\n        oof_test = np.zeros((X.shape[0], len(self.mod)))\n        for i, model in enumerate(self.saved_model):\n            predictions = np.column_stack([m.predict(X) for m in model])\n            oof_test[:, i] = np.mean(predictions, axis=1)\n        return self.meta_model.predict(oof_test)\n\nstack_model = Stacking(mod=[lasso, ridge, svr, ker, ela, bay], meta_model=ker)\n\n# Fit the stacking model\nstack_model.fit(X_train_preprocessed.toarray(), y_log_series)\n\n# Predictions from stacking model\nstack_pred_log = stack_model.predict(X_test_preprocessed.toarray())\n\n# Inverse transform the stacking predictions\nstack_pred = np.expm1(stack_pred_log)\n\n# Bagging VotingRegressor with ExtraTreesRegressor\net = ExtraTreesRegressor()\n\n# Create VotingRegressor\nbagging_regressor = VotingRegressor(estimators=[('Extra Trees', et)])\n\n# Fit Bagging model\nbagging_regressor.fit(X_train_preprocessed.toarray(), y_log_series)\n\n# Predictions from bagging model\nbagging_pred_log = bagging_regressor.predict(X_test_preprocessed.toarray())\n\n# Inverse transform the bagging predictions\nbagging_pred = np.expm1(bagging_pred_log)\n\n# Boosting AdaBoostRegressor\nada_boost = AdaBoostRegressor()\n\n# Fit boosting model\nada_boost.fit(X_train_preprocessed.toarray(), y_log_series)\n\n# Predictions from boosting model\nboosting_pred_log = ada_boost.predict(X_test_preprocessed.toarray())\n\n# Inverse transform the boosting predictions\nboosting_pred = np.expm1(boosting_pred_log)\n\n# Ensure all predictions have the same shape\nprint(f\"Shape of stack_pred: {stack_pred.shape}\")\nprint(f\"Shape of bagging_pred: {bagging_pred.shape}\")\nprint(f\"Shape of boosting_pred: {boosting_pred.shape}\")\n\n# Calculate ensemble prediction from all models\nensemble_pred = (stack_pred * 0.903 +\n                 bagging_pred * 0.048 +\n                 boosting_pred * 0.049)\n\n# Prepare submission\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble_pred\n\n# Check final submission shape\nprint(f\"Submission shape: {sub.shape}\")\n\n# Save submission to CSV\nsub.to_csv('submission.csv', index=False)\n\n# Read the CSV file into a DataFrame\nsubmission = pd.read_csv('submission.csv')\n\n# Display the first few rows of the DataFrame\nprint(submission.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-06T17:25:22.259792Z","iopub.execute_input":"2024-07-06T17:25:22.260154Z","iopub.status.idle":"2024-07-06T17:25:31.911532Z","shell.execute_reply.started":"2024-07-06T17:25:22.260124Z","shell.execute_reply":"2024-07-06T17:25:31.910222Z"},"trusted":true},"execution_count":null,"outputs":[]}]}